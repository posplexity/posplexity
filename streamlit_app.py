from src.llm.deepseek.inference import run_deepseek_stream, run_deepseek
from src.llm.gpt.inference import run_gpt
from src.search.search import search 
from src.utils.utils import async_wrapper
import streamlit as st
import streamlit.components.v1 as components
import asyncio, json
from common.types import intlist_struct
try:
    loop = asyncio.get_event_loop()
except RuntimeError:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

name_source_mapping = json.load(open("data/mapping.json", "r"))

def setup_sidebar():
    """
    ì‚¬ì´ë“œë°” UIë¥¼ êµ¬ì„±í•˜ê³ , ì „ì—­ ë³€ìˆ˜ì— ëª¨ë¸ ì„ íƒ/ì˜µì…˜ì„ ì„¸íŒ…í•œë‹¤.
    """
    try:
        st.sidebar.image(
            "data/assets/postech_logo.svg",
            use_container_width=True
        )
    except:
        st.sidebar.image(
            "data/assets/postech_logo.svg",
            use_column_width=True
        )

    st.sidebar.markdown("""
    \nìƒˆë‚´ê¸° ì—¬ëŸ¬ë¶„ì˜ ê¶ê¸ˆì¦ì„ í•´ì†Œí•˜ê¸° ìœ„í•´ ê´€ë ¨ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
    """)

    # ì˜ˆì‹œ ì§ˆë¬¸ ì„¹ì…˜
    with st.sidebar.expander("â„¹ï¸ ì˜ˆì‹œ ì§ˆë¬¸", expanded=True):
        example_questions = [
            "ë°¥ì•½ì´ ë¬´ìŠ¨ ëœ»ì¸ê°€ìš”?",
            "ìƒˆí„° ê¸°ê°„ë™ì•ˆ ìˆ ì„ ë§ˆì…”ë„ ê´œì°®ë‚˜ìš”?",
            "ì•¼êµ¬ë¥¼ ì¢‹ì•„í•˜ëŠ”ë°, ì–´ë–¤ ë™ì•„ë¦¬ì— ë“¤ì–´ê°€ëŠ”ê²Œ ì¢‹ì„ê¹Œìš”?",
        ]
        for question in example_questions:
            if st.button(question):
                st.session_state.pending_question = question
                st.rerun()

    st.sidebar.divider()

    with st.sidebar.expander("ğŸ’¬ ë¬¸ì˜í•˜ê¸°", expanded=False):
        st.markdown("""                    
            ### Contact
            ê¶ê¸ˆí•œ ì ì´ë‚˜ í”¼ë“œë°±ì€ ì–¸ì œë“ ì§€ ì•„ë˜ í˜ì´ì§€ë¥¼ í†µí•´ ê³µìœ í•´ ì£¼ì„¸ìš”.
            - [ë¬¸ì˜ì‚¬í•­ í˜ì´ì§€](https://forms.gle/aMAJA7yPFfCRGLro9)
                    
            ### Contributing
            ìë£Œë¥¼ ë³´ì™„í•˜ê±°ë‚˜ ìƒˆë¡­ê²Œ ì¶”ê°€í•˜ê³  ì‹¶ì€ ë‚´ìš©ì´ ìˆë‹¤ë©´, ì•„ë˜ ì—…ë¡œë“œ í˜ì´ì§€ë¥¼ ì´ìš©í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.
            - [ì—…ë¡œë“œ í˜ì´ì§€](https://docs.google.com/forms/d/e/1FAIpQLScUW14gj69mWXlhoKpJejBLWCbj-wOQZ4e6XQT69ZFNWZS4SA/viewform)
        """)

    with st.sidebar.expander("ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ì œì‘ì", expanded=False):
        st.markdown("""
            ### Contributers
            [**í—ˆì±„ì›**](https://www.linkedin.com/in/cwhuh/)(í¬ìŠ¤í… 24),  
            [**ìµœì§€ì•ˆ**](https://www.linkedin.com/in/%EC%A7%80%EC%95%88-%EC%B5%9C-72093030a/)(í¬ìŠ¤í… 24),  
            [**ìµœì£¼ì—°**](https://www.linkedin.com/in/%EC%A3%BC%EC%97%B0-%EC%B5%9C-a9884331b/)(í¬ìŠ¤í… 24),  
            [**ì •ì°¬í¬**](https://www.linkedin.com/in/%EC%B0%AC%ED%9D%AC-%EC%A0%95-b6506b328/)(í¬ìŠ¤í… 24)
        """)

    with st.sidebar.expander("ğŸ’» ì½”ë“œ", expanded=False):
        st.markdown("""
            ì „ì²´ ì½”ë“œëŠ” ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.  
            [**GitHub**](https://github.com/chaewon-huh/posplexity)
        """)


def setup_page():
    """
    ë©”ì¸ í˜ì´ì§€(ë³¸ë¬¸) ì„¤ì •ì„ ë‹´ë‹¹. íƒ€ì´í‹€, ë¶€ê°€ ë¬¸êµ¬ ë“±ì„ í‘œì‹œ.
    """
    st.title("POSTECH 25í•™ë²ˆ ì…í•™ì„ í™˜ì˜í•©ë‹ˆë‹¤!")
    st.caption("powered by P13")


# Streamlit Settings
st.set_page_config(page_title="Posplexity", layout="wide")

# ì‚¬ì´ë“œë°”ì™€ í˜ì´ì§€ êµ¬ì„±
setup_sidebar()
setup_page()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []  # [{role: "user"/"assistant", content: "..."}]


# ê¸°ì¡´ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


# ì˜ˆì‹œ ì§ˆë¬¸ ì²˜ë¦¬ - pending_question
prompt = None

# (a) ë¨¼ì €, ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼ í´ë¦­ìœ¼ë¡œ ì €ì¥ëœ pending_questionì´ ìˆìœ¼ë©´ ì‚¬ìš©
if "pending_question" in st.session_state:
    prompt = st.session_state.pending_question
    del st.session_state.pending_question  # í•œ ë²ˆ ì‚¬ìš© í›„ ì‚­ì œ

# (b) ì‚¬ìš©ìê°€ ì§ì ‘ ì…ë ¥í•œ ì±„íŒ…ì´ ìˆìœ¼ë©´ ê·¸ê±¸ë¡œ ëŒ€ì²´
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
if user_input:
    prompt = user_input

if prompt:
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 2. ì±—ë´‡ ì‘ë‹µ (LLM í˜¸ì¶œ)
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        reference_placeholder = st.empty()  # ì¶œì²˜ í‘œì‹œìš©

        async def get_response():
            """
            ì‚¬ìš©ì ì§ˆì˜ë¥¼ ë°›ì•„ì„œ,
            1. RAG ê²€ìƒ‰ (top_k=20)
            2. LLM Re-ranking -> ìƒìœ„ ID ì¶”ì¶œ
            3. ìµœì¢… RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ ë‹µë³€ (ìŠ¤íŠ¸ë¦¬ë°)
            """
            try:
                # (1) ëŒ€í™” íˆìŠ¤í† ë¦¬ ì •ë¦¬
                history_text = ""
                for msg in st.session_state.messages[:-1]:
                    if msg["role"] == "user":
                        history_text += f"User: {msg['content']}\n"
                    elif msg["role"] == "assistant":
                        history_text += f"Assistant: {msg['content']}\n"

                # (2) RAG ê²€ìƒ‰
                found_chunks = []
                with st.spinner("ë¬¸ì„œë¥¼ ì¡°íšŒ ì¤‘ì…ë‹ˆë‹¤..."):
                    found_chunks = search(prompt, top_k=20, dev=False)

                # (3) Re-ranking
                # ê° ì²­í¬: c["id"], c["doc_title"], c["raw_text"], c["doc_source"], c["page_num"] ...
                # 3-1. (id, text_summary) í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
                chunk_dict = {
                    c["id"]: (c["doc_title"], c["summary"])
                    for c in found_chunks
                }

                # 3-2. Re-rankingì„ ìœ„í•´ LLM í˜¸ì¶œ (ìŠ¤í”¼ë„ˆ ì¶”ê°€)
                with st.spinner("ë¬¸ì„œë¥¼ ì¬ì •ë ¬ ì¤‘ì…ë‹ˆë‹¤..."):
                    reranked_chunks = run_gpt(
                        target_prompt=str(chunk_dict),       # LLMì— ë„˜ê¸¸ ë¬¸ìì—´ (id -> title & ìš”ì•½)
                        prompt_in_path="reranking.json",     # rerankingë¥¼ ìˆ˜í–‰í•˜ëŠ” JSON prompt
                        gpt_model="gpt-4o-2024-08-06",
                        output_structure=intlist_struct
                    )

                reranked_ids = reranked_chunks.output

                # (4) re-rankedëœ idì— í•´ë‹¹í•˜ëŠ” ì²­í¬ë§Œ ì¶”ì¶œ
                filtered_chunks = [c for c in found_chunks if c["id"] in reranked_ids]

                # (4-1) re-ranked ë¦¬ìŠ¤íŠ¸ ìˆœì„œ ìœ ì§€ ìœ„í•´ id -> index ë§¤í•‘
                id_to_rank = {id_: idx for idx, id_ in enumerate(reranked_ids)}
                sorted_chunks = sorted(filtered_chunks, key=lambda x: id_to_rank[x["id"]])

                # (4-2) ìµœì¢… RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                context_texts = [c["raw_text"] for c in sorted_chunks]
                rag_context = "\n".join(context_texts)
                final_prompt = f"""
ì•„ë˜ëŠ” ì´ì „ ëŒ€í™”ì˜ ê¸°ë¡ì…ë‹ˆë‹¤:
{history_text}

ë‹¤ìŒì€ ì°¸ê³  ìë£Œ(RAG)ì—ì„œ ë°œì·Œí•œ ë‚´ìš©ì…ë‹ˆë‹¤:
{rag_context}

ì´ì œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤:

ì§ˆë¬¸: {prompt}

ìœ„ ëŒ€í™”ì™€ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.
ë‹µë³€:
"""

                # (5) ìµœì¢… ë‹µë³€ (ìŠ¤íŠ¸ë¦¬ë°)
                stream = await run_deepseek_stream(
                    target_prompt=final_prompt,
                    prompt_in_path="chat_basic.json"
                )

                full_response = ""
                async for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        full_response += chunk.choices[0].delta.content
                        message_placeholder.markdown(full_response)

                # (6) ì¶œì²˜ í‘œì‹œ - ìµœì¢… ì‚¬ìš©ëœ sorted_chunks ê¸°ì¤€
                if sorted_chunks:
                    dedup_set = set()
                    for c in sorted_chunks:
                        doc_title = c.get("doc_title", "Untitled")
                        doc_source = c.get("doc_source", "Unknown Source")
                        # name_source_mappingì—ì„œ ë§¤í•‘
                        if not doc_source.startswith("http"):
                            doc_source = name_source_mapping.get(doc_title, doc_source)
                        page_num = c.get("page_num", None)
                        dedup_set.add((doc_title, doc_source, page_num))

                    refs = []
                    for idx, (title, source, page) in enumerate(dedup_set, start=1):
                        if source.startswith("http"):
                            if page is not None:
                                refs.append(f"- **{title}** (p.{page}) / [ë§í¬ë¡œ ì´ë™]({source})")
                            else:
                                refs.append(f"- **{title}** / [ë§í¬ë¡œ ì´ë™]({source})")
                        else:
                            if page is not None:
                                refs.append(f"- **{title}** (p.{page}) / {source}")
                            else:
                                refs.append(f"- **{title}** / {source}")

                    refs_text = "\n".join(refs)
                    reference_placeholder.markdown(
                        f"---\n**ì°¸ê³  ìë£Œ ì¶œì²˜**\n\n{refs_text}\n"
                    )

                return full_response

            except Exception as e:
                raise Exception(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

        try:
            # ë¹„ë™ê¸° ì‘ë‹µ ì²˜ë¦¬
            response = loop.run_until_complete(get_response())
            st.session_state.messages.append({
                "role": "assistant",
                "content": response
            })
        except Exception as e:
            message_placeholder.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")