# streamlit_app.py
import streamlit as st
import asyncio
import os
import streamlit.components.v1 as components  # iFrame ì„ë² ë“œìš©

from src.llm.deepseek.inference import run_deepseek_stream
from src.llm.gpt.inference import run_gpt_stream
from src.search.search import search  # Qdrant ë²¡í„° ê²€ìƒ‰ í•¨ìˆ˜

try:
    loop = asyncio.get_event_loop()
except RuntimeError:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

# ì „ì—­ ë³€ìˆ˜ì²˜ëŸ¼ ì“¸ ìˆ˜ ìˆë„ë¡ ì´ê³³ì—ì„œ ì„ ì–¸ (í˜¹ì€ setup_sidebarì—ì„œ ë°˜í™˜ë°›ì•„ë„ ë¨)
model_choice = None
use_rag = True

def setup_sidebar():
    """
    ì‚¬ì´ë“œë°” UIë¥¼ êµ¬ì„±í•˜ê³ , ì „ì—­ ë³€ìˆ˜ì— ëª¨ë¸ ì„ íƒ/ì˜µì…˜ì„ ì„¸íŒ…í•œë‹¤.
    """
    global model_choice, use_rag

    ########################################
    # (1) ì‚¬ì´ë“œë°” ìµœìƒë‹¨ ë¡œê³  í‘œì‹œ
    ########################################
    st.sidebar.image(
        "assets/postech_logo.svg",
        use_column_width=True
    )
    # --------------------------------------

    st.sidebar.markdown("""
    \nìƒˆë‚´ê¸°ë“¤ì˜ ë¶ˆí¸í•¨ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´, ê·¼ê±°ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì±—ë´‡ì„ ì œì‘í•˜ì˜€ìŠµë‹ˆë‹¤.
    """)

    # ì˜ˆì‹œ ì§ˆë¬¸ ì„¹ì…˜
    with st.sidebar.expander("â„¹ï¸ ì˜ˆì‹œ ì§ˆë¬¸", expanded=True):
        example_questions = [
            "ë°¥ì•½ì´ ë¬´ìŠ¨ ëœ»ì¸ê°€ìš”?",
            "1í•™ë…„ ê¸°ìˆ™ì‚¬ì—ì„œ ìˆ ì„ ë§ˆì‹¤ ìˆ˜ ìˆë‚˜ìš”?",
            "í¬ìŠ¤í… ë°´ë“œ ë™ì•„ë¦¬ì—ëŠ” ì–´ë–¤ê²Œ ìˆë‚˜ìš”?",
        ]
        for question in example_questions:
            if st.button(question):
                # ì§ˆë¬¸ì„ ì„¸ì…˜ì— ì €ì¥, rerun í›„ mainì—ì„œ ì²˜ë¦¬
                st.session_state.pending_question = question
                st.rerun()

    st.sidebar.divider()

    # ë¬¸ì˜í•˜ê¸°
    with st.sidebar.expander("ğŸ’¬ ë¬¸ì˜í•˜ê¸°", expanded=False):
        st.markdown("""
            ### Contact
            ì‘ë‹µ ë¬¸ì„œ ë° ìë£Œ ì œë³´, ì¶”ê°€ ê¸°ëŠ¥ ì œì•ˆ, í”¼ë“œë°± ì‚¬ì•ˆì€ ëª¨ë‘ í•˜ê¸° ì´ë©”ì¼ë¡œ ì •ë¦¬í•˜ì—¬ ë³´ë‚´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.
            - cw.huh@postech.ac.kr
        """)

    # ì œì‘ì
    with st.sidebar.expander("ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ì œì‘ì", expanded=False):
        st.markdown("""
            ### Contributers
            [**í—ˆì±„ì›**](https://www.linkedin.com/in/cwhuh/)(í¬ìŠ¤í… 24),  
            [**ìµœì§€ì•ˆ**](https://www.linkedin.com/in/%EC%A7%80%EC%95%88-%EC%B5%9C-72093030a/)(í¬ìŠ¤í… 24),  
            [**ìµœì£¼ì—°**](https://www.linkedin.com/in/%EC%A3%BC%EC%97%B0-%EC%B5%9C-a9884331b/)(í¬ìŠ¤í… 24),  
            [**ì •ì°¬í¬**](https://www.linkedin.com/in/%EC%B0%AC%ED%9D%AC-%EC%A0%95-b6506b328/)(í¬ìŠ¤í… 24)
        """)

    # ì½”ë“œ
    with st.sidebar.expander("ğŸ’» ì½”ë“œ", expanded=False):
        st.markdown("""
            ì „ì²´ ì½”ë“œëŠ” ê³µê°œë˜ì–´ ìˆìœ¼ë©°, ììœ ë¡œìš´ í™œìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.  
            [**GitHub**](https://github.com/chaewon-huh/posplexity)
        """)

    # (í•„ìš”í•˜ë©´ ëª¨ë¸ ì„ íƒ, RAG ì˜µì…˜ ë³µêµ¬)
    # model_choice = st.sidebar.radio(
    #     "ëª¨ë¸ ì„ íƒ",
    #     ["DeepSeek", "GPT"],
    #     captions=["DeepSeek-v3", "gpt-4o-mini (ë¹„ì¶”ì²œ)"]
    # )
    # use_rag = st.sidebar.checkbox("Use RAG", value=True, help="ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì°¸ê³ ")


def setup_page():
    """
    ë©”ì¸ í˜ì´ì§€(ë³¸ë¬¸) ì„¤ì •ì„ ë‹´ë‹¹. íƒ€ì´í‹€, ë¶€ê°€ ë¬¸êµ¬ ë“±ì„ í‘œì‹œ.
    """
    st.title("POSTECH 25í•™ë²ˆ ì…í•™ì„ í™˜ì˜í•©ë‹ˆë‹¤!")
    st.caption("powered by P13")


#############################################
# Streamlit ê¸°ë³¸ ì„¤ì •
#############################################
st.set_page_config(page_title="Posplexity", layout="wide")

# ë¨¼ì € ì‚¬ì´ë“œë°”ì™€ í˜ì´ì§€ êµ¬ì„±
setup_sidebar()
setup_page()

#############################################
# (1) ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
#############################################
if "messages" not in st.session_state:
    st.session_state.messages = []  # [{role: "user"/"assistant", content: "..."}]

#############################################
# (2) ê¸°ì¡´ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
#############################################
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

#############################################
# (3) ì˜ˆì‹œ ì§ˆë¬¸ ì²˜ë¦¬ - pending_question
#############################################
prompt = None

# (a) ë¨¼ì €, ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼ í´ë¦­ìœ¼ë¡œ ì €ì¥ëœ pending_questionì´ ìˆìœ¼ë©´ ì‚¬ìš©
if "pending_question" in st.session_state:
    prompt = st.session_state.pending_question
    del st.session_state.pending_question  # í•œ ë²ˆ ì‚¬ìš© í›„ ì‚­ì œ

# (b) ì‚¬ìš©ìê°€ ì§ì ‘ ì…ë ¥í•œ ì±„íŒ…ì´ ìˆìœ¼ë©´ ê·¸ê±¸ë¡œ ëŒ€ì²´
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
if user_input:
    prompt = user_input

#############################################
# (4) promptê°€ ìµœì¢… ê²°ì •ë˜ë©´ -> ëª¨ë¸ í˜¸ì¶œ
#############################################
if prompt:
    # 1) ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 2) ì±—ë´‡ ì‘ë‹µ (LLM í˜¸ì¶œ)
    with st.chat_message("assistant"):
        # ë©”ì‹œì§€ ì¶œë ¥ ì˜ì—­ 2ê°œ: (1) ëª¨ë¸ ë‹µë³€ìš©, (2) ì¶œì²˜ í‘œì‹œìš©
        message_placeholder = st.empty()
        reference_placeholder = st.empty()  # ì¶œì²˜ í‘œì‹œìš©

        async def get_response():
            """
            ì‚¬ìš©ì ì§ˆì˜ë¥¼ ë°›ì•„ì„œ,
            1) (ì˜µì…˜) RAG ê²€ìƒ‰
            2) ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ + (ì˜µì…˜) RAG ì»¨í…ìŠ¤íŠ¸ -> LLMì— ì „ë‹¬ (ìŠ¤íŠ¸ë¦¬ë°)
            3) ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ë°˜í™˜
            """
            try:
                # (a) ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
                history_text = ""
                # ë§ˆì§€ë§‰(í˜„ì¬ ë°œí™”í•œ user ë©”ì‹œì§€)ì€ ì œì™¸í•˜ê³  í•©ì¹¨
                for msg in st.session_state.messages[:-1]:
                    if msg["role"] == "user":
                        history_text += f"User: {msg['content']}\n"
                    elif msg["role"] == "assistant":
                        history_text += f"Assistant: {msg['content']}\n"
                
                # (b) RAG ê²€ìƒ‰(ì˜µì…˜) -- use_rag, model_choiceê°€ ì£¼ì„ ì²˜ë¦¬ë˜ì–´ìˆìœ¼ë‹ˆ
                #    ê¸°ë³¸ê°’(True)ë¡œ ë‘ê±°ë‚˜ í•„ìš”ì— ë§ê²Œ ìˆ˜ì •
                found_chunks = []
                if use_rag:
                    with st.spinner("ë¬¸ì„œ íƒìƒ‰ ì¤‘..."):
                        found_chunks = search(prompt, top_k=5)  # Qdrant ë²¡í„° ê²€ìƒ‰
                
                # ê²€ìƒ‰ëœ ì²­í¬ë“¤ì„ í•©ì³ì„œ RAG ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                context_texts = [c["raw_text"] for c in found_chunks]
                rag_context = "\n".join(context_texts)

                # (c) ìµœì¢… Prompt ìƒì„±
                final_prompt = f"""
ì•„ë˜ëŠ” ì´ì „ì— ì§„í–‰ëœ ëŒ€í™”ì…ë‹ˆë‹¤:
{history_text}

ê·¸ë¦¬ê³  ì•„ë˜ëŠ” RAG ê²€ìƒ‰ì—ì„œ ì°¾ì€ ì°¸ê³  ìë£Œì…ë‹ˆë‹¤:
{rag_context}

ì´ì œ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë‹¤ì‹œ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤:

ì§ˆë¬¸: {prompt}

ìœ„ ëŒ€í™”ì™€ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.
ë‹µë³€:
"""

                # (d) LLMì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬ (ìŠ¤íŠ¸ë¦¬ë°)
                # model_choiceê°€ Noneì¼ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë‹ˆ, ê¸°ë³¸ê°’ ì²˜ë¦¬
                selected_model = model_choice if model_choice else "DeepSeek"

                if selected_model == "GPT":
                    stream = await run_gpt_stream(
                        target_prompt=final_prompt,
                        prompt_in_path="chat_basic.json"
                    )
                else:  # "DeepSeek"
                    stream = await run_deepseek_stream(
                        target_prompt=final_prompt,
                        prompt_in_path="chat_basic.json"
                    )
                
                # (e) ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ì²˜ë¦¬ (ë©”ì‹œì§€ ëˆ„ì í•˜ì—¬ í‘œì‹œ)
                full_response = ""
                async for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        full_response += chunk.choices[0].delta.content
                        message_placeholder.markdown(full_response)

                # (f) ê²€ìƒ‰ëœ ì²­í¬ì˜ ì¶œì²˜ ë§Œë“¤ê¸° (ì˜µì…˜)
                if use_rag and found_chunks:
                    dedup_set = set()
                    for c in found_chunks:
                        doc_source = c.get("doc_source", "Unknown Source")
                        doc_title = c.get("doc_title", "Untitled")
                        page_num = c.get("page_num", None)  # PDF í˜ì´ì§€ ë²ˆí˜¸
                        dedup_set.add((doc_title, doc_source, page_num))

                    refs = []
                    for idx, (title, source, page) in enumerate(dedup_set, start=1):
                        if page is not None:
                            refs.append(f"- **{title}** (p.{page}) / {source}")
                        else:
                            refs.append(f"- **{title}** / {source}")
                    
                    refs_text = "\n".join(refs)
                    reference_placeholder.markdown(
                        f"---\n**ì°¸ê³  ë¬¸ì„œ(ì²­í¬) ì¶œì²˜**\n\n{refs_text}\n"
                    )

                return full_response

            except Exception as e:
                raise Exception(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        try:
            # ë¹„ë™ê¸° ì‘ë‹µ ì²˜ë¦¬
            response = loop.run_until_complete(get_response())
            st.session_state.messages.append({
                "role": "assistant",
                "content": response
            })
        except Exception as e:
            message_placeholder.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")