from src.llm.deepseek.inference import run_deepseek_stream
from src.llm.gpt.inference import run_gpt_stream
from src.search.search import search 

import streamlit as st
import streamlit.components.v1 as components
import asyncio


try:
    loop = asyncio.get_event_loop()
except RuntimeError:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)


def setup_sidebar():
    """
    ì‚¬ì´ë“œë°” UIë¥¼ êµ¬ì„±í•˜ê³ , ì „ì—­ ë³€ìˆ˜ì— ëª¨ë¸ ì„ íƒ/ì˜µì…˜ì„ ì„¸íŒ…í•œë‹¤.
    """
    try:
        st.sidebar.image(
            "assets/postech_logo.svg",
            use_container_width=True
        )
    except:
        st.sidebar.image(
            "assets/postech_logo.svg",
            use_column_width=True
        )

    st.sidebar.markdown("""
    \nìƒˆë‚´ê¸° ì—¬ëŸ¬ë¶„ì˜ ê¶ê¸ˆì¦ì„ í•´ì†Œí•˜ê¸° ìœ„í•´ ê´€ë ¨ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
    """)

    # ì˜ˆì‹œ ì§ˆë¬¸ ì„¹ì…˜
    with st.sidebar.expander("â„¹ï¸ ì˜ˆì‹œ ì§ˆë¬¸", expanded=True):
        example_questions = [
            "ë°¥ì•½ì´ ë¬´ìŠ¨ ëœ»ì¸ê°€ìš”?",
            "1í•™ë…„ ê¸°ìˆ™ì‚¬ì—ì„œ ìˆ ì„ ë§ˆì‹¤ ìˆ˜ ìˆë‚˜ìš”?",
            "í¬ìŠ¤í… ë°´ë“œ ë™ì•„ë¦¬ì—ëŠ” ì–´ë–¤ê²Œ ìˆë‚˜ìš”?",
        ]
        for question in example_questions:
            if st.button(question):
                st.session_state.pending_question = question
                st.rerun()

    st.sidebar.divider()

    with st.sidebar.expander("ğŸ’¬ ë¬¸ì˜í•˜ê¸°", expanded=False):
        st.markdown("""                    
            ### Contact
            ê°œì„  ì‚¬í•­ì´ë‚˜ í”¼ë“œë°±ì€ ì•„ë˜ ì´ë©”ì¼ë¡œ ë³´ë‚´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.
            - postech.p13@gmail.com
                    
            ### Contributing
            ë³´ì¶©í•  ìë£Œê°€ ìˆìœ¼ë©´ ì–¸ì œë“  ê³µìœ  ë¶€íƒë“œë¦½ë‹ˆë‹¤.
            - [ì—…ë¡œë“œ í˜ì´ì§€](https://docs.google.com/forms/d/e/1FAIpQLScUW14gj69mWXlhoKpJejBLWCbj-wOQZ4e6XQT69ZFNWZS4SA/viewform)
        """)

    with st.sidebar.expander("ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ì œì‘ì", expanded=False):
        st.markdown("""
            ### Contributers
            [**í—ˆì±„ì›**](https://www.linkedin.com/in/cwhuh/)(í¬ìŠ¤í… 24),  
            [**ìµœì§€ì•ˆ**](https://www.linkedin.com/in/%EC%A7%80%EC%95%88-%EC%B5%9C-72093030a/)(í¬ìŠ¤í… 24),  
            [**ìµœì£¼ì—°**](https://www.linkedin.com/in/%EC%A3%BC%EC%97%B0-%EC%B5%9C-a9884331b/)(í¬ìŠ¤í… 24),  
            [**ì •ì°¬í¬**](https://www.linkedin.com/in/%EC%B0%AC%ED%9D%AC-%EC%A0%95-b6506b328/)(í¬ìŠ¤í… 24)
        """)

    with st.sidebar.expander("ğŸ’» ì½”ë“œ", expanded=False):
        st.markdown("""
            ì „ì²´ ì½”ë“œëŠ” ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.  
            [**GitHub**](https://github.com/chaewon-huh/posplexity)
        """)


def setup_page():
    """
    ë©”ì¸ í˜ì´ì§€(ë³¸ë¬¸) ì„¤ì •ì„ ë‹´ë‹¹. íƒ€ì´í‹€, ë¶€ê°€ ë¬¸êµ¬ ë“±ì„ í‘œì‹œ.
    """
    st.title("POSTECH 25í•™ë²ˆ ì…í•™ì„ í™˜ì˜í•©ë‹ˆë‹¤!")
    st.caption("powered by P13")


# Streamlit Settings
st.set_page_config(page_title="Posplexity", layout="wide")

# ì‚¬ì´ë“œë°”ì™€ í˜ì´ì§€ êµ¬ì„±
setup_sidebar()
setup_page()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []  # [{role: "user"/"assistant", content: "..."}]


# ê¸°ì¡´ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


# ì˜ˆì‹œ ì§ˆë¬¸ ì²˜ë¦¬ - pending_question
prompt = None

# (a) ë¨¼ì €, ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼ í´ë¦­ìœ¼ë¡œ ì €ì¥ëœ pending_questionì´ ìˆìœ¼ë©´ ì‚¬ìš©
if "pending_question" in st.session_state:
    prompt = st.session_state.pending_question
    del st.session_state.pending_question  # í•œ ë²ˆ ì‚¬ìš© í›„ ì‚­ì œ

# (b) ì‚¬ìš©ìê°€ ì§ì ‘ ì…ë ¥í•œ ì±„íŒ…ì´ ìˆìœ¼ë©´ ê·¸ê±¸ë¡œ ëŒ€ì²´
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
if user_input:
    prompt = user_input

if prompt:
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 2. ì±—ë´‡ ì‘ë‹µ (LLM í˜¸ì¶œ)
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        reference_placeholder = st.empty()  # ì¶œì²˜ í‘œì‹œìš©

        async def get_response():
            """
            ì‚¬ìš©ì ì§ˆì˜ë¥¼ ë°›ì•„ì„œ,
            1. RAG ê²€ìƒ‰
            2. ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ + RAG ì»¨í…ìŠ¤íŠ¸ -> LLMì— ì „ë‹¬ (ìŠ¤íŠ¸ë¦¬ë°)
            3. ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ë°˜í™˜
            """
            try:
                history_text = ""
                # ë§ˆì§€ë§‰(í˜„ì¬ ë°œí™”í•œ user ë©”ì‹œì§€)ì€ ì œì™¸í•˜ê³  í•©ì¹¨
                for msg in st.session_state.messages[:-1]:
                    if msg["role"] == "user":
                        history_text += f"User: {msg['content']}\n"
                    elif msg["role"] == "assistant":
                        history_text += f"Assistant: {msg['content']}\n"
                
                # 1. RAG ê²€ìƒ‰
                found_chunks = []
                with st.spinner("ë¬¸ì„œë¥¼ ì¡°íšŒ ì¤‘ì…ë‹ˆë‹¤..."):
                    found_chunks = search(prompt, top_k=8, dev=False)  # Qdrant ë²¡í„° ê²€ìƒ‰
                
                # 2-1. ê²€ìƒ‰ëœ ì²­í¬ë“¤ì„ í•©ì³ ìµœì¢… Prompt êµ¬ì„±
                context_texts = [c["raw_text"] for c in found_chunks]
                rag_context = "\n".join(context_texts)
                final_prompt = f"""
ì•„ë˜ëŠ” ì´ì „ ëŒ€í™”ì˜ ê¸°ë¡ì…ë‹ˆë‹¤:
{history_text}

ë‹¤ìŒì€ ì°¸ê³  ìë£Œ(RAG)ì—ì„œ ë°œì·Œí•œ ë‚´ìš©ì…ë‹ˆë‹¤:
{rag_context}

ì´ì œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤:

ì§ˆë¬¸: {prompt}

ìœ„ ëŒ€í™”ì™€ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.
ë‹µë³€:
"""

                # 2-2. LLMì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬ (ìŠ¤íŠ¸ë¦¬ë°)
                stream = await run_deepseek_stream(
                    target_prompt=final_prompt,
                    prompt_in_path="chat_basic.json"
                )
                
                # 3. ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ì²˜ë¦¬
                full_response = ""
                async for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        full_response += chunk.choices[0].delta.content
                        message_placeholder.markdown(full_response)

                # ì¶œì²˜ í‘œì‹œ
                if found_chunks:
                    dedup_set = set()
                    for c in found_chunks:
                        doc_source = c.get("doc_source", "Unknown Source")
                        doc_title = c.get("doc_title", "Untitled")
                        page_num = c.get("page_num", None)
                        dedup_set.add((doc_title, doc_source, page_num))

                    refs = []
                    for idx, (title, source, page) in enumerate(dedup_set, start=1):
                        if page is not None:
                            refs.append(f"- **{title}** (p.{page}) / {source}")
                        else:
                            refs.append(f"- **{title}** / {source}")
                    
                    refs_text = "\n".join(refs)
                    reference_placeholder.markdown(
                        f"---\n**ì°¸ê³  ìë£Œ ì¶œì²˜**\n\n{refs_text}\n"
                    )

                return full_response

            except Exception as e:
                raise Exception(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

        try:
            # ë¹„ë™ê¸° ì‘ë‹µ ì²˜ë¦¬
            response = loop.run_until_complete(get_response())
            st.session_state.messages.append({
                "role": "assistant",
                "content": response
            })
        except Exception as e:
            message_placeholder.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")